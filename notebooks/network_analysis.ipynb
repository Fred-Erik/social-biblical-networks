{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse network centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads the graph data generated by the co-occurence reading and calculates centrality using different measures, which are then stored to csv-files in two formats: per chapter and for a whole book per name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path to where the graphs are stored\n",
    "path = \"c:/Frederik/Afstuderen/Results/test07/\"\n",
    "# what centrality measures to use\n",
    "centrality_measures = ['eigenvector_centrality', 'katz_centrality']\n",
    "#['betweenness_centrality', 'closeness_centrality', 'degree_centrality', 'eigenvector_centrality', 'katz_centrality']\n",
    "\n",
    "# export csv files to-location\n",
    "exportpath = \"{}analysis/\".format(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Bible_Genesis_13.gexf\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import operator\n",
    "import re\n",
    "import csv\n",
    "import collections\n",
    "import community\n",
    "\n",
    "if not os.path.exists(exportpath):\n",
    "    os.makedirs(exportpath)\n",
    "\n",
    "graphs = []\n",
    "chapterlist = []\n",
    "bookgraph = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".gexf\"):\n",
    "        match = re.search(r'Bible(_([\\w]+))?\\.gexf', file)\n",
    "        if match is not None:\n",
    "            graphs.append([match.group(2), nx.read_gexf(path+file, relabel=True)])\n",
    "            print(\"Imported {}\".format(file))\n",
    "            \n",
    "            chaptermatch = re.search(r'_(\\d+)', file)\n",
    "            bookmatch = re.search(r'Bible_([A-Za-z_]+)\\.gexf', file)\n",
    "            \n",
    "            if bookmatch is not None:\n",
    "                print('Using {} as bookgraph, so adding community stuff there'.format(match.group(0)))\n",
    "                bookgraph = [match.group(2), nx.read_gexf(path+file, relabel=True)]\n",
    "            elif chaptermatch is not None:\n",
    "                chapterlist.append(int(chaptermatch.group(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated eigenvector_centrality\n",
      "Calculated katz_centrality\n"
     ]
    }
   ],
   "source": [
    "centralities = collections.defaultdict(lambda: collections.defaultdict(lambda:0))\n",
    "centralities_sorted = collections.defaultdict(lambda:[])\n",
    "\n",
    "for centrality_measure_name in centrality_measures:\n",
    "    centrality_measure = getattr(nx, centrality_measure_name)\n",
    "    \n",
    "    for graphlist in graphs:\n",
    "        graphname = graphlist[0]\n",
    "        graph = graphlist[1]\n",
    "        \n",
    "        try:\n",
    "            if centrality_measure_name == 'closeness_centrality':\n",
    "                centrality = centrality_measure(graph, distance=\"weight\")\n",
    "            elif centrality_measure_name == 'degree_centrality':\n",
    "                centrality = centrality_measure(graph)\n",
    "            else:\n",
    "                centrality = centrality_measure(graph, weight=\"weight\")\n",
    "\n",
    "            centralities[centrality_measure_name][graphname] = centrality\n",
    "            centrality_sorted = sorted(centrality.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            centralities_sorted[centrality_measure_name].append([graphname, centrality_sorted])\n",
    "        except nx.NetworkXError:\n",
    "            print('Failed to converge on graph {}'.format(graphname))\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Division by zero on graph {}\".format(graphname))\n",
    "            \n",
    "    print(\"Calculated {}\".format(centrality_measure_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate network properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated graph density, average clustering coefficient and Freeman centralization\n"
     ]
    }
   ],
   "source": [
    "network_properties = []\n",
    "header = ['graph', 'density', 'avg_clust']\n",
    "for centrality_measure_name in centrality_measures:\n",
    "    header.append('centralization_' + centrality_measure_name)\n",
    "network_properties.append(header)\n",
    "\n",
    "#density = []\n",
    "#avg_clust = []\n",
    "#centralization = collections.defaultdict(lambda: [])\n",
    "\n",
    "for graphlist in graphs:\n",
    "    graphname = graphlist[0]\n",
    "    graph = graphlist[1]\n",
    "    \n",
    "    network_properties_row = []\n",
    "    network_properties_row.append(graphname)\n",
    "    \n",
    "    # calculate density for this graph\n",
    "    this_density = round(nx.density(graph),5)\n",
    "    network_properties_row.append(this_density)\n",
    "    #density.append([graphname, this_density])\n",
    "    \n",
    "    # calculate average clustering coefficient for this graph\n",
    "    this_avg_clust = round(nx.average_clustering(graph),5)\n",
    "    network_properties_row.append(this_avg_clust)\n",
    "    #avg_clust.append([graphname, this_avg_clust])\n",
    "    \n",
    "    # calculate Freeman centralization for every centrality measure for this grpah\n",
    "    for centrality_measure_name in centrality_measures:\n",
    "        N = graph.order()\n",
    "        try:\n",
    "            all_cen = centralities[centrality_measure_name][graphname].values()\n",
    "        except AttributeError:\n",
    "            # this happens when the centrality could not be calculated in the previous section\n",
    "            continue\n",
    "        max_cen = max(all_cen)\n",
    "        try:\n",
    "            this_centralization = round(float((N*max_cen - sum(all_cen)))/(N-1)**2,5)\n",
    "        except ZeroDivisionError:\n",
    "            # this seems to happen when the network is too small (smaller than 2?)\n",
    "            print('Failed to calculate Freeman centralization for graph {}'.format(graphname))\n",
    "            continue\n",
    "            \n",
    "        #centralization[centrality_measure_name].append([graphname, this_centralization])\n",
    "        network_properties_row.append(this_centralization)\n",
    "\n",
    "    network_properties.append(network_properties_row)\n",
    "    \n",
    "print(\"Calculated graph density, average clustering coefficient and Freeman centralization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEXF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add 'community' attribute to book graph and export GEXF\n",
    "if bookgraph:\n",
    "    graphname = bookgraph[0]\n",
    "    graph = bookgraph[1]\n",
    "\n",
    "    part = community.best_partition(graph)\n",
    "    nx.set_node_attributes(graph, 'community', part)\n",
    "    graphcommunity = community.induced_graph(part, graph)\n",
    "    nx.write_gexf(graph, \"{}Bible_{}_comattr.gexf\".format(exportpath, graphname))\n",
    "    nx.write_gexf(graphcommunity, \"{}Bible_{}_comnodes.gexf\".format(exportpath, graphname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully exported pergraph_katz_centrality.csv\n",
      "Succesfully exported pername_katz_centrality.csv\n",
      "Succesfully exported pergraph_eigenvector_centrality.csv\n",
      "Succesfully exported pername_eigenvector_centrality.csv\n",
      "Succesfully exported network_properties.csv\n"
     ]
    }
   ],
   "source": [
    "for centrality_measure_name, centrality_dict in centralities_sorted.items():\n",
    "    # Create graph-table\n",
    "    with open(\"{}pergraph_{}.csv\".format(exportpath, centrality_measure_name), mode=\"w\", newline=\"\\n\") as table:\n",
    "        writer = csv.writer(table)\n",
    "        for centrality_dictitem in centrality_dict:\n",
    "            for item in centrality_dictitem[1]:\n",
    "                writer.writerow((centrality_dictitem[0], item[0], round(item[1],5)))\n",
    "\n",
    "        print(\"Succesfully exported pergraph_{}.csv\".format(centrality_measure_name))\n",
    "        \n",
    "    # Create graph over time-table\n",
    "    namelist = []\n",
    "    for centrality_dictitem in centrality_dict:\n",
    "        for item in centrality_dictitem[1]:\n",
    "            if item[0] not in namelist:\n",
    "                namelist.append(item[0])\n",
    "   \n",
    "    try:\n",
    "        names_centralities = [[0 for _ in range(max(chapterlist)+1)] for _ in range (len(namelist))]\n",
    "        for idx_name, name in enumerate(namelist):\n",
    "            names_centralities[idx_name][0] = name\n",
    "            for centrality_dictitem in centrality_dict:\n",
    "                try:\n",
    "                    chapternr = int(re.search(r'_(\\d+)', centrality_dictitem[0]).group(1))\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "\n",
    "                for item in centrality_dictitem[1]:\n",
    "                    if item[0] == name:\n",
    "                        if item[1] > 0.001:\n",
    "                            names_centralities[idx_name][chapternr] = round(item[1],5)\n",
    "            names_centralities[idx_name].append(round(sum(names_centralities[idx_name][1:]),5))\n",
    "\n",
    "        with open(\"{}pername_{}.csv\".format(exportpath, centrality_measure_name), mode=\"w\", newline=\"\\n\") as table:\n",
    "            writer = csv.writer(table)\n",
    "            for row in names_centralities:\n",
    "                writer.writerow(row)\n",
    "\n",
    "            print(\"Succesfully exported pername_{}.csv\".format(centrality_measure_name))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "# Create network properties-table\n",
    "with open(\"{}network_properties.csv\".format(exportpath), mode=\"w\", newline=\"\\n\") as table:\n",
    "    writer = csv.writer(table)\n",
    "    for network_properties_row in network_properties:\n",
    "        writer.writerow(network_properties_row)\n",
    "        \n",
    "    print(\"Succesfully exported network_properties.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
