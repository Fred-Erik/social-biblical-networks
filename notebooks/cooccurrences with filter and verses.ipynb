{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create co-occurence networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates co-occurence networks and exports them to .gexf-files, either at the start of a new book or chapter or one network for all selected books and chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which Bible passages to create co-occurence networks for\n",
    "# -1 matches the last chapter/verse. Useful when selecting a whole book/chapter\n",
    "passages = {\n",
    "    \"Samuel_I\": [1,1,-1,-1]\n",
    "}\n",
    "\n",
    "# what range the co-occurence networks should have\n",
    "# creates new file and network at the start of a new \"chapter\", \"book\" or never when set to \"bible\"\n",
    "network_range = \"book\"\n",
    "\n",
    "# include / exclude named entitity types:\n",
    "# \"pers\" = person\n",
    "# \"mens\" = measurement unit\n",
    "# \"gens\" = people\n",
    "# \"topo\" = place\n",
    "# \"ppde\" = demonstrative personal pronoun\n",
    "# \"\"     = not specified (seems to be the 'gentillic' words, i.e. from which country someone is)\n",
    "allowed_nametypes = [\"pers\"]\n",
    "#allowed_nametypes = [\"pers\", \"mens\", \"gens\", \"topo\", \"ppde\", \"\"]\n",
    "\n",
    "# minimum weight an edge should have before it is added to the output file\n",
    "min_edge_weight = 0.1\n",
    "\n",
    "# formula for the weight between words dependent on the distance in amount of sentences\n",
    "# weight = 1/(distance in amount of sentences^2)\n",
    "def get_weight(src_sentence, tgt_sentence):\n",
    "    return 1/(abs(src_sentence - tgt_sentence)+1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the python modules, the plot modules, the LAF-Fabric module (``laf``) and initialize the ``laf`` processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.4\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from laf.fabric import LafFabric\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.02s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.02s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "    14s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/Judices/__log__Judices.txt\n",
      "    14s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK Judices AT 2016-03-30T14-46-40\n"
     ]
    }
   ],
   "source": [
    "fabric.load('etcbc4', 'lexicon', '_'.join(passages.keys()),\n",
    "{\n",
    "    \"primary\": False,\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": (\"otype book chapter verse number lex_utf8 gloss sp ls nametype\", \"\"),\n",
    "},)\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_header = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<gexf xmlns:viz=\"http:///www.gexf.net/1.2draft/viz\" xmlns=\"http://www.gexf.net/1.1draft\" version=\"1.2\">\n",
    "<meta>\n",
    "<creator>LAF-Fabric</creator>\n",
    "</meta>\n",
    "<graph defaultedgetype=\"undirected\" idtype=\"string\" type=\"static\">\n",
    "<attributes class=\"node\" mode=\"static\">\n",
    "<attribute id=\"occurrence\" title=\"occurrence\" type=\"string\"></attribute>\n",
    "<attribute id=\"nametype\" title=\"nametype\" type=\"string\"></attribute>\n",
    "</attributes>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace -1 in 'passages' dict with inf\n",
    "for b, cv in passages.items():\n",
    "    for idx, item in enumerate(cv):\n",
    "        if item == -1:\n",
    "            cv[idx] = float('inf')\n",
    "    passages[b] = cv\n",
    "    \n",
    "books = []\n",
    "unique_labels = []\n",
    "unique_nodes = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda:0))))\n",
    "def get_unique_lexeme(book_name, chapter_nr, lexeme):\n",
    "    if network_range == \"chapter\":\n",
    "        return \"{}_{}_{}\".format(book_name, chapter_nr, lexeme)\n",
    "    elif network_range == \"book\":\n",
    "        return \"{}_{}\".format(book_name, lexeme)\n",
    "    else:\n",
    "        return lexeme\n",
    "\n",
    "correct_verse = False\n",
    "node_id = 0\n",
    "lexemes = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda:collections.defaultdict(lambda:0))))\n",
    "edge_weight = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk through the relevant nodes and collect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 14s END\n",
      "    14s Done\n"
     ]
    }
   ],
   "source": [
    "for node in NN():\n",
    "    this_type = F.otype.v(node)\n",
    "\n",
    "    # when arriving at a new verse, check if that verse is in the allowed passages\n",
    "    if this_type == \"verse\":\n",
    "        correct_verse = False\n",
    "        this_book = F.book.v(node)\n",
    "\n",
    "        for b, cv in passages.items():\n",
    "            this_chapter = int(F.chapter.v(node))\n",
    "\n",
    "            if this_book == b and this_chapter >= cv[0] and this_chapter <= cv[2]:\n",
    "                this_verse = int(F.verse.v(node))\n",
    "                \n",
    "                if cv[0] == cv[2]:\n",
    "                    if this_verse >= cv[1] and this_verse <= cv[3]:\n",
    "                        correct_verse = True\n",
    "                        break\n",
    "                elif (\n",
    "                    (this_chapter == cv[0] and this_verse >= cv[1]) or\n",
    "                    (this_chapter > cv[0] and this_chapter < cv[2]) or\n",
    "                    (this_chapter == cv[2] and this_verse <= cv[3])\n",
    "                   ):\n",
    "                    \n",
    "                    correct_verse = True\n",
    "                    break\n",
    "                \n",
    "    # if the verse is allowed, continue\n",
    "    if correct_verse:\n",
    "        if this_book not in books:\n",
    "            books.append(this_book)\n",
    "            \n",
    "        if this_type == \"sentence\":\n",
    "            this_sentence = int(F.number.v(node))\n",
    "        \n",
    "        if this_type == \"word\":\n",
    "            lexeme = F.lex_utf8.v(node)\n",
    "            nametype = F.nametype.v(node)\n",
    "            allowed_nametype = any(x in nametype.split(',') for x in allowed_nametypes)\n",
    "            unique_lexeme = get_unique_lexeme(this_book, this_chapter, lexeme)\n",
    "\n",
    "            # if part of speech == proper noun or lexical set == gentilic and nametype is allowed\n",
    "            if (F.sp.v(node) == 'nmpr' or F.ls.v(node) == 'gntl') and allowed_nametype:\n",
    "                lexemes[this_book][this_chapter][this_sentence][lexeme] += 1\n",
    "                occurrence = \"{}_{},\".format(this_book, this_chapter)\n",
    "\n",
    "                # if the word is not yet in the chapter / book / bible, add it\n",
    "                if unique_lexeme not in unique_nodes:\n",
    "                    unique_nodes[unique_lexeme][\"id\"] = node_id\n",
    "                    node_id += 1\n",
    "\n",
    "                    english_name = re.sub(r'\\W+', '', F.gloss.v(node))\n",
    "                    \n",
    "                    if [lexeme, english_name] not in unique_labels:\n",
    "                        english_names = [x[1] for x in unique_labels]\n",
    "                        while english_name in english_names and [lexeme, english_name] not in unique_labels:\n",
    "                            english_name += \"2\"\n",
    "                        if [lexeme, english_name] not in unique_labels:\n",
    "                            unique_labels.append([lexeme, english_name])\n",
    "                    \n",
    "                    unique_nodes[unique_lexeme][\"gloss\"] = english_name\n",
    "                    unique_nodes[unique_lexeme][\"nametype\"] = nametype\n",
    "                    unique_nodes[unique_lexeme][\"occurrence\"] = occurrence\n",
    "                # otherwise add the occurence information\n",
    "                else:\n",
    "                    if not unique_nodes[unique_lexeme][\"occurrence\"].endswith(occurrence):\n",
    "                        unique_nodes[unique_lexeme][\"occurrence\"] += occurrence\n",
    "\n",
    "msg(\"Done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# show what's in a dict in a readable way (show first 1000 char)\n",
    "import json\n",
    "print(json.dumps(lexemes, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the data according to the various subtasks, and compute the edges with their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    16s chapter:  21; nodes:  89; edges: 385\n",
      "    16s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/Judices\n",
      "\n",
      "Bible_Judices.gexf                    47570 Wed Mar 30 16:46:56 2016\n",
      "__log__Judices.txt                      282 Wed Mar 30 16:46:40 2016\n"
     ]
    }
   ],
   "source": [
    "last_book = len(books) - 1\n",
    "for idx_book, book_name in enumerate(books):\n",
    "    \n",
    "    last_chapter = len(lexemes[book_name]) - 1\n",
    "    for idx_chapter, chapter_nr in enumerate(lexemes[book_name]):\n",
    "\n",
    "        # create the nodes in the graph\n",
    "        node_data = []\n",
    "        unique_labels = []\n",
    "        for node in unique_nodes:\n",
    "            if (\n",
    "                (network_range == \"chapter\" and \"{}_{}_\".format(book_name, chapter_nr) in node) or\n",
    "                (network_range == \"book\" and \"{}_\".format(book_name) in node) or\n",
    "                (network_range != \"chapter\" and network_range != \"book\")\n",
    "                ):\n",
    "\n",
    "                node_data.append('''<node id=\"{}\" label=\"{}\">\\n  <attvalues><attvalue for=\"occurrence\" value=\"{}\"/><attvalue for=\"nametype\" value=\"{}\"/></attvalues>\\n</node>\\n'''.format(\n",
    "                    unique_nodes[node][\"id\"], unique_nodes[node][\"gloss\"], unique_nodes[node][\"occurrence\"], unique_nodes[node][\"nametype\"]))\n",
    "\n",
    "        # read all lexemes from the chapter and add them to the list 'names'\n",
    "        names = []\n",
    "        for sentence in lexemes[book_name][chapter_nr]:\n",
    "            words = lexemes[book_name][chapter_nr][sentence].keys()\n",
    "            # add sentence id to words like this: [sentence number, word]\n",
    "            # so the distance between the words can be calculated\n",
    "            words = [[s,sentence] for s in words]\n",
    "            names.extend(words)\n",
    "\n",
    "        # calculate the edge weights between the names in 'names'\n",
    "        for src in range(len(names)):\n",
    "            src_sentence = names[src][1]\n",
    "            lexemes_src = names[src][0]\n",
    "            \n",
    "            for tgt in range(src + 1, len(names)):\n",
    "                tgt_sentence = names[tgt][1]\n",
    "                lexemes_tgt = names[tgt][0]\n",
    "                \n",
    "                if lexemes_src != lexemes_tgt:\n",
    "                    # set weight dependent on how much sentences the words are away from each other\n",
    "                    this_weight = get_weight(src_sentence, tgt_sentence)\n",
    "\n",
    "                    occurrence = \"{}_{},\".format(book_name,chapter_nr)\n",
    "                    \n",
    "                    # if there already exists a relation the other way around, swap vars\n",
    "                    if edge_weight[lexemes_tgt][lexemes_src][\"weight\"] > 0:\n",
    "                        if not edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"]:\n",
    "                            edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"] = occurrence\n",
    "                        elif not edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"].endswith(occurrence):\n",
    "                            edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"] += occurrence\n",
    "\n",
    "                        edge_weight[lexemes_tgt][lexemes_src][\"weight\"] += this_weight\n",
    "                    else:\n",
    "                        if not edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"]:\n",
    "                            edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"] = occurrence\n",
    "                        elif not edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"].endswith(occurrence):\n",
    "                            edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"] += occurrence\n",
    "                                \n",
    "                        edge_weight[lexemes_src][lexemes_tgt][\"weight\"] += this_weight\n",
    "\n",
    "        # if a network is complete, create the edges, export the file and clear the edge weights\n",
    "        if (\n",
    "                (network_range == \"chapter\") or\n",
    "                (network_range == \"book\" and idx_chapter == last_chapter) or\n",
    "                (idx_chapter == last_chapter and idx_book == last_book)\n",
    "            ):\n",
    "            \n",
    "            # create the edges\n",
    "            edge_id = 0\n",
    "            edge_data = []\n",
    "            for src in edge_weight:\n",
    "                unique_lexeme_src = get_unique_lexeme(book_name, chapter_nr, src)\n",
    "\n",
    "                for tgt in edge_weight[src]:\n",
    "                    unique_lexeme_tgt = get_unique_lexeme(book_name, chapter_nr, tgt)\n",
    "\n",
    "                    if edge_weight[src][tgt][\"weight\"] > min_edge_weight:\n",
    "                        edge_id += 1\n",
    "\n",
    "                        edge_data.append('''<edge id=\"{}\" source=\"{}\" target=\"{}\" weight=\"{}\" label=\"{}\"/>\\n'''.\n",
    "                        format(edge_id, unique_nodes[unique_lexeme_src][\"id\"], unique_nodes[unique_lexeme_tgt][\"id\"], round(edge_weight[src][tgt][\"weight\"],2), edge_weight[src][tgt][\"occurrence\"]))\n",
    "\n",
    "            # export the file\n",
    "            edges_header = '''<edges count=\"{}\">\\n'''.format(len(edge_data))\n",
    "\n",
    "            if network_range == \"chapter\":\n",
    "                filename = 'Bible_{}_{}.gexf'.format(book_name, str(chapter_nr).zfill(2))\n",
    "            elif network_range == \"book\":\n",
    "                filename = 'Bible_{}.gexf'.format(book_name)\n",
    "            else:\n",
    "                filename = 'Bible.gexf'\n",
    "            out_file = outfile(filename)\n",
    "            \n",
    "            out_file.write(data_header)\n",
    "\n",
    "            nodes_header = '''<nodes count=\"{}\">\\n\\n'''.format(len(node_data))\n",
    "            out_file.write(nodes_header)\n",
    "            for node_line in node_data:\n",
    "                out_file.write(node_line)\n",
    "            out_file.write(\"</nodes>\\n\")\n",
    "\n",
    "            out_file.write(edges_header)\n",
    "            for edge_line in edge_data:\n",
    "                out_file.write(edge_line)\n",
    "            out_file.write(\"</edges>\\n\")\n",
    "            out_file.write(\"</graph></gexf>\\n\")\n",
    "\n",
    "            msg(\"chapter:  {}; nodes:  {}; edges: {}\".format(chapter_nr, len(node_data), len(edge_data)))\n",
    "             \n",
    "            # clear the edge weights for the next network\n",
    "            edge_weight = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: 0)))\n",
    "close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
