{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create co-occurence networks test 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allpassages = [\"\"\"{\n",
    "    \"Samuel_I\": [27, 1, 27,-1],\n",
    "    \"Chronica_I\": [12, 1, 12, 7]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_I\": [29, 1, 29,3],\n",
    "    \"Chronica_I\": [12, 19, 12, 22]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_I\": [31, 1, 31,-1],\n",
    "    \"Chronica_I\": [10, 1, 10, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [5, 1, 5,5],\n",
    "    \"Chronica_I\": [11, 1, 11, 3]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [5, 6, 5,10],\n",
    "    \"Chronica_I\": [11, 4, 11, 9]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [5, 11, 5,16],\n",
    "    \"Chronica_I\": [14, 1, 14, 7]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [5, 17, 5,25],\n",
    "    \"Chronica_I\": [14, 8, 14, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [6, 1, 6,11],\n",
    "    \"Chronica_I\": [13, 1, 13, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [11, 1, 11,27],\n",
    "    \"Chronica_I\": [20, 1, 20, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [12, 29, 12,31],\n",
    "    \"Chronica_I\": [20, 1, 20, 3]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [23, 8, 23,39],\n",
    "    \"Chronica_I\": [11, 10, 11, 47]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [24, 1, 24,9],\n",
    "    \"Chronica_I\": [21, 1, 21, 6]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [24, 10, 24,17],\n",
    "    \"Chronica_I\": [21, 7, 21, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [2, 1, 2,1],\n",
    "    \"Chronica_I\": [23, 1, 23, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [2, 10, 2,12],\n",
    "    \"Chronica_I\": [29, 23, 29, 30]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [2, 46, 2,46],\n",
    "    \"Chronica_II\": [1, 1, 1, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [3, 4, 3,15],\n",
    "    \"Chronica_II\": [1, 2, 1, 13]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [5, 1, 5,-1],\n",
    "    \"Chronica_II\": [2, 1, 2, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [7, 15, 7,21],\n",
    "    \"Chronica_II\": [3, 15, 3, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [7, 23, 7,26],\n",
    "    \"Chronica_II\": [4, 2, 4, 5]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [7, 47, 7,50],\n",
    "    \"Chronica_II\": [4, 18, 4, 22]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [7, 51, 7,51],\n",
    "    \"Chronica_II\": [5, 1, 5, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [9, 1, 9,9],\n",
    "    \"Chronica_II\": [7, 11, 7, 22]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [9, 10, 9,28],\n",
    "    \"Chronica_II\": [8, 1, 8, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [10, 1, 10,13],\n",
    "    \"Chronica_II\": [9, 1, 9, 12]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [10, 14, 10,25],\n",
    "    \"Chronica_II\": [9, 13, 9, 24]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [10, 26, 10,29],\n",
    "    \"Chronica_II\": [9, 25, 9, 28]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [10, 26, 10,29],\n",
    "    \"Chronica_II\": [1, 14, 1, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [11, 41, 11,43],\n",
    "    \"Chronica_II\": [9, 29, 9, 31]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [12, 1, 12,19],\n",
    "    \"Chronica_II\": [10, 1, 10, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [12, 21, 12,24],\n",
    "    \"Chronica_II\": [11, 1, 11, 4]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [12, 25, 12,25],\n",
    "    \"Chronica_II\": [11, 5, 11, 12]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [12, 26, 12,31],\n",
    "    \"Chronica_II\": [11, 13, 11, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [14, 22, 14,24],\n",
    "    \"Chronica_II\": [12, 1, 12, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [14, 25, 14,28],\n",
    "    \"Chronica_II\": [12, 2, 12, 12]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 6, 15,6],\n",
    "    \"Chronica_II\": [13, 2, 13, 21]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 13, 15,15],\n",
    "    \"Chronica_II\": [15, 16, 15, 18]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 16, 15,22],\n",
    "    \"Chronica_II\": [16, 1, 16, 6]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [22, 45, 22,45],\n",
    "    \"Chronica_II\": [20, 34, 20, 34]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [22, 47, 22,49],\n",
    "    \"Chronica_II\": [20, 35, 20, 37]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [22, 50, 22,50],\n",
    "    \"Chronica_II\": [21, 1, 21, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [8, 16, 8,19],\n",
    "    \"Chronica_II\": [21, 2, 21, 7]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [8, 20, 8,22],\n",
    "    \"Chronica_II\": [21, 8, 21, 15]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [8, 25, 8,27],\n",
    "    \"Chronica_II\": [22, 1, 22, 4]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [10, 11, 10,14],\n",
    "    \"Chronica_II\": [22, 8, 22, 8]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [11, 1, 11,3],\n",
    "    \"Chronica_II\": [22, 10, 22, 12]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [11, 4, 11,20],\n",
    "    \"Chronica_II\": [23, 1, 23, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [12, 6, 12,16],\n",
    "    \"Chronica_II\": [24, 4, 24, 14]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [12, 19, 12,21],\n",
    "    \"Chronica_II\": [24, 25, 24, 27]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [14, 1, 14,6],\n",
    "    \"Chronica_II\": [25, 1, 25, 4]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [14, 7, 14,7],\n",
    "    \"Chronica_II\": [25, 11, 25, 16]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [14, 8, 14,14],\n",
    "    \"Chronica_II\": [25, 17, 25, 24]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [14, 17, 14,20],\n",
    "    \"Chronica_II\": [25, 25, 25, 28]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [15, 32, 15,35],\n",
    "    \"Chronica_II\": [27, 1, 27, 8]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [15, 38, 15,38],\n",
    "    \"Chronica_II\": [27, 9, 27, 9]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [16, 7, 16,7],\n",
    "    \"Chronica_II\": [28, 16, 28, 19]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [15, 29, 15,29],\n",
    "    \"Chronica_II\": [28, 20, 28, 20]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [16, 8, 16,18],\n",
    "    \"Chronica_II\": [28, 21, 28, 25]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [18, 13, 18,13],\n",
    "    \"Jesaia\": [36, 1, 36, 1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [18, 14, 18,16],\n",
    "    \"Chronica_II\": [32, 2, 32, 8]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [20, 1, 20,11],\n",
    "    \"Chronica_II\": [32, 24, 32, 24]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [20, 1, 20,11],\n",
    "    \"Jesaia\": [38, 1, 38, -1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [20, 12, 20,19],\n",
    "    \"Jesaia\": [39, 1, 39, 8]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [18, 17, 18,37],\n",
    "    \"Chronica_II\": [32, 9, 32, 19]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [18, 17, 18,37],\n",
    "    \"Jesaia\": [36, 2, 36, 22]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 1, 19,5],\n",
    "    \"Chronica_II\": [32, 20, 32, 20]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 1, 19,5],\n",
    "    \"Jesaia\": [37, 1, 37, 4]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 8, 19,19],\n",
    "    \"Chronica_II\": [32, 17, 32, 17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 8, 19,19],\n",
    "    \"Jesaia\": [37, 8, 37, 20]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 20, 19,37],\n",
    "    \"Chronica_II\": [32, 21, 32, 21]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 20, 19,37],\n",
    "    \"Jesaia\": [37, 21, 37, 38]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [21, 1, 21,16],\n",
    "    \"Chronica_II\": [33, 1, 33, 9]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [21, 19, 21,26],\n",
    "    \"Chronica_II\": [33, 21, 33, 25]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [22, 3, 22,20],\n",
    "    \"Chronica_II\": [34, 8, 34, 28]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 1, 23,3],\n",
    "    \"Chronica_II\": [34, 29, 34, 32]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 21, 23,23],\n",
    "    \"Chronica_II\": [35, 1, 35, 19]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 24, 23,26],\n",
    "    \"Chronica_II\": [34, 33, 34, 33]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 28, 23,30],\n",
    "    \"Chronica_II\": [35, 20, 35, 27]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 30, 23,33],\n",
    "    \"Chronica_II\": [36, 1, 36, 3]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [24, 15, 24,17],\n",
    "    \"Chronica_II\": [36, 10, 36, 10]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [24, 20, 24,20],\n",
    "    \"Chronica_II\": [36, 13, 36, 16]\n",
    "}\"\"\"]\n",
    "\n",
    "allpassagesmanual = [\"\"\"{\n",
    "    \"Samuel_II\": [6, 12, 6,23],\n",
    "    \"Chronica_I\": [15, 1, 16,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [7, 1, 7,-1],\n",
    "    \"Chronica_I\": [17, 1, 17,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [8, 1, 8,-1],\n",
    "    \"Chronica_I\": [18, 1, 18,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [10, 1, 10,-1],\n",
    "    \"Chronica_I\": [19, 1, 19,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [24, 1, 24,9],\n",
    "    \"Chronica_I\": [27, 23, 27,24]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Samuel_II\": [24, 18, 24,24],\n",
    "    \"Chronica_I\": [21, 18, 22,1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [2, 1, 2,4],\n",
    "    \"Chronica_I\": [28, 28, 28,21]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [6, 1, 6,-1],\n",
    "    \"Chronica_I\": [3, 1, 3,14]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [7, 38, 7,46],\n",
    "    \"Chronica_II\": [4, 6, 4,17]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [8, 1, 8,-1],\n",
    "    \"Chronica_II\": [5, 2, 7,10]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [14, 21, 14,31],\n",
    "    \"Chronica_II\": [12, 13, 12,16]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 1, 15,-1],\n",
    "    \"Chronica_II\": [13, 1, 13,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 1, 15,-1],\n",
    "    \"Chronica_II\": [13, 1, 13,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 1, 15,-1],\n",
    "    \"Chronica_II\": [14, 1, 14,10]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [15, 1, 15,-1],\n",
    "    \"Chronica_II\": [16, 10, 16,14]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [22, 1, 22,44],\n",
    "    \"Chronica_II\": [18, 1, 18,-1]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [22, 40, 22,44],\n",
    "    \"Chronica_II\": [20, 30, 20,33]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_I\": [3, 4, 3,5],\n",
    "    \"Chronica_II\": [20, 1, 20,3]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [8, 23, 8,24],\n",
    "    \"Chronica_II\": [21, 18, 21,20]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [8, 28, 9,28],\n",
    "    \"Chronica_II\": [22, 5, 22,9]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [11, 21, 12,3],\n",
    "    \"Chronica_II\": [24, 1, 24,3]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [12, 17, 12,18],\n",
    "    \"Chronica_II\": [24, 23, 24,24]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [14, 21, 15,4],\n",
    "    \"Chronica_II\": [26, 1, 26,15]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [15, 6, 15,28],\n",
    "    \"Chronica_II\": [26, 22, 26,23]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [16, 1, 16,2],\n",
    "    \"Chronica_II\": [28, 1, 28,2]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [16, 3, 16,6],\n",
    "    \"Chronica_II\": [28, 3, 28,8]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [16, 19, 16,20],\n",
    "    \"Chronica_II\": [28, 26, 28,27]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [18, 1, 18,3],\n",
    "    \"Chronica_II\": [29, 1, 29,2]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [19, 6, 19,7],\n",
    "    \"Jesaia\": [37, 6, 37,7]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [20, 20, 20,21],\n",
    "    \"Chronica_II\": [32, 32, 32,33]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [21, 17, 21,18],\n",
    "    \"Chronica_II\": [33, 18, 33,20]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [22, 1, 22,2],\n",
    "    \"Chronica_II\": [34, 1, 34,7]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [23, 34, 23,37],\n",
    "    \"Chronica_II\": [36, 4, 36,5]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [24, 8, 24,9],\n",
    "    \"Chronica_II\": [36, 9, 36,9]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [24, 18, 24,19],\n",
    "    \"Chronica_II\": [36, 11, 36,12]\n",
    "}\"\"\",\"\"\"{\n",
    "    \"Reges_II\": [25, 8, 25,21],\n",
    "    \"Chronica_II\": [36, 18, 36,21]\n",
    "}\"\"\"]\n",
    "\n",
    "allpassages.extend(allpassagesmanual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for passages in allpassages:\n",
    "    counter +=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.4\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
      "\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.02s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  4.48s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/001/__log__001.txt\n",
      "  4.48s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 001 AT 2016-03-24T11-00-49\n",
      "  2.33s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/001\n",
      "\n",
      "Bible_Chronica_I.gexf                 24910 Thu Mar 24 12:00:51 2016\n",
      "Bible_Samuel_I.gexf                   10856 Thu Mar 24 12:00:51 2016\n",
      "__log__001.txt                          210 Thu Mar 24 12:00:49 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/002/__log__002.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 002 AT 2016-03-24T11-00-51\n",
      "  2.29s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/002\n",
      "\n",
      "Bible_Chronica_I.gexf                  5980 Thu Mar 24 12:00:53 2016\n",
      "Bible_Samuel_I.gexf                    3389 Thu Mar 24 12:00:53 2016\n",
      "__log__002.txt                          210 Thu Mar 24 12:00:51 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/003/__log__003.txt\n",
      "  0.03s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 003 AT 2016-03-24T11-00-53\n",
      "  2.24s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/003\n",
      "\n",
      "Bible_Chronica_I.gexf                  5459 Thu Mar 24 12:00:56 2016\n",
      "Bible_Samuel_I.gexf                    4686 Thu Mar 24 12:00:56 2016\n",
      "__log__003.txt                          210 Thu Mar 24 12:00:53 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.01s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/004/__log__004.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 004 AT 2016-03-24T11-00-56\n",
      "  2.27s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/004\n",
      "\n",
      "Bible_Chronica_I.gexf                  2319 Thu Mar 24 12:00:58 2016\n",
      "Bible_Samuel_II.gexf                   2880 Thu Mar 24 12:00:58 2016\n",
      "__log__004.txt                          210 Thu Mar 24 12:00:56 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/005/__log__005.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 005 AT 2016-03-24T11-00-58\n",
      "  2.26s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/005\n",
      "\n",
      "Bible_Chronica_I.gexf                  3471 Thu Mar 24 12:01:00 2016\n",
      "Bible_Samuel_II.gexf                   1669 Thu Mar 24 12:01:00 2016\n",
      "__log__005.txt                          210 Thu Mar 24 12:00:58 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/006/__log__006.txt\n",
      "  0.03s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 006 AT 2016-03-24T11-01-00\n",
      "  2.24s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/006\n",
      "\n",
      "Bible_Chronica_I.gexf                 10376 Thu Mar 24 12:01:02 2016\n",
      "Bible_Samuel_II.gexf                  10321 Thu Mar 24 12:01:02 2016\n",
      "__log__006.txt                          210 Thu Mar 24 12:01:00 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/007/__log__007.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 007 AT 2016-03-24T11-01-02\n",
      "  2.36s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/007\n",
      "\n",
      "Bible_Chronica_I.gexf                  3026 Thu Mar 24 12:01:05 2016\n",
      "Bible_Samuel_II.gexf                   3102 Thu Mar 24 12:01:05 2016\n",
      "__log__007.txt                          210 Thu Mar 24 12:01:02 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/008/__log__008.txt\n",
      "  0.03s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 008 AT 2016-03-24T11-01-05\n",
      "  2.58s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/008\n",
      "\n",
      "Bible_Chronica_I.gexf                  7424 Thu Mar 24 12:01:07 2016\n",
      "Bible_Samuel_II.gexf                   4684 Thu Mar 24 12:01:07 2016\n",
      "__log__008.txt                          210 Thu Mar 24 12:01:05 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/009/__log__009.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 009 AT 2016-03-24T11-01-08\n",
      "  2.23s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/009\n",
      "\n",
      "Bible_Chronica_I.gexf                  1934 Thu Mar 24 12:01:10 2016\n",
      "Bible_Samuel_II.gexf                   5591 Thu Mar 24 12:01:10 2016\n",
      "__log__009.txt                          210 Thu Mar 24 12:01:08 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/010/__log__010.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 010 AT 2016-03-24T11-01-10\n",
      "  2.18s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/010\n",
      "\n",
      "Bible_Chronica_I.gexf                  2065 Thu Mar 24 12:01:12 2016\n",
      "Bible_Samuel_II.gexf                   1455 Thu Mar 24 12:01:12 2016\n",
      "__log__010.txt                          210 Thu Mar 24 12:01:10 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/011/__log__011.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 011 AT 2016-03-24T11-01-12\n",
      "  2.44s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/011\n",
      "\n",
      "Bible_Chronica_I.gexf                 89321 Thu Mar 24 12:01:14 2016\n",
      "Bible_Samuel_II.gexf                 189592 Thu Mar 24 12:01:14 2016\n",
      "__log__011.txt                          210 Thu Mar 24 12:01:12 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/012/__log__012.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 012 AT 2016-03-24T11-01-14\n",
      "  2.18s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/012\n",
      "\n",
      "Bible_Chronica_I.gexf                  4321 Thu Mar 24 12:01:17 2016\n",
      "Bible_Samuel_II.gexf                   9569 Thu Mar 24 12:01:17 2016\n",
      "__log__012.txt                          210 Thu Mar 24 12:01:14 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/013/__log__013.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 013 AT 2016-03-24T11-01-17\n",
      "  2.21s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/013\n",
      "\n",
      "Bible_Chronica_I.gexf                  2728 Thu Mar 24 12:01:19 2016\n",
      "Bible_Samuel_II.gexf                   3294 Thu Mar 24 12:01:19 2016\n",
      "__log__013.txt                          210 Thu Mar 24 12:01:17 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/014/__log__014.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 014 AT 2016-03-24T11-01-19\n",
      "  2.32s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/014\n",
      "\n",
      "Bible_Chronica_I.gexf                  1240 Thu Mar 24 12:01:21 2016\n",
      "Bible_Reges_I.gexf                      913 Thu Mar 24 12:01:21 2016\n",
      "__log__014.txt                          210 Thu Mar 24 12:01:19 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/015/__log__015.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 015 AT 2016-03-24T11-01-21\n",
      "  2.19s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/015\n",
      "\n",
      "Bible_Chronica_I.gexf                  4107 Thu Mar 24 12:01:24 2016\n",
      "Bible_Reges_I.gexf                     1950 Thu Mar 24 12:01:24 2016\n",
      "__log__015.txt                          210 Thu Mar 24 12:01:21 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/016/__log__016.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 016 AT 2016-03-24T11-01-24\n",
      "  2.21s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/016\n",
      "\n",
      "Bible_Chronica_II.gexf                 1228 Thu Mar 24 12:01:26 2016\n",
      "Bible_Reges_I.gexf                     1068 Thu Mar 24 12:01:26 2016\n",
      "__log__016.txt                          210 Thu Mar 24 12:01:24 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/017/__log__017.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 017 AT 2016-03-24T11-01-26\n",
      "  2.20s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/017\n",
      "\n",
      "Bible_Chronica_II.gexf                 5665 Thu Mar 24 12:01:28 2016\n",
      "Bible_Reges_I.gexf                     2016 Thu Mar 24 12:01:28 2016\n",
      "__log__017.txt                          210 Thu Mar 24 12:01:26 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.01s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/018/__log__018.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 018 AT 2016-03-24T11-01-28\n",
      "  2.24s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/018\n",
      "\n",
      "Bible_Chronica_II.gexf                 4777 Thu Mar 24 12:01:30 2016\n",
      "Bible_Reges_I.gexf                     9494 Thu Mar 24 12:01:30 2016\n",
      "__log__018.txt                          210 Thu Mar 24 12:01:28 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.01s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/019/__log__019.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 019 AT 2016-03-24T11-01-30\n",
      "  2.20s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/019\n",
      "\n",
      "Bible_Chronica_II.gexf                  918 Thu Mar 24 12:01:33 2016\n",
      "Bible_Reges_I.gexf                      906 Thu Mar 24 12:01:33 2016\n",
      "__log__019.txt                          210 Thu Mar 24 12:01:30 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/020/__log__020.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 020 AT 2016-03-24T11-01-33\n",
      "  2.24s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/020\n",
      "\n",
      "__log__020.txt                          210 Thu Mar 24 12:01:33 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/021/__log__021.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 021 AT 2016-03-24T11-01-35\n",
      "  2.17s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/021\n",
      "\n",
      "Bible_Chronica_II.gexf                  695 Thu Mar 24 12:01:37 2016\n",
      "Bible_Reges_I.gexf                      912 Thu Mar 24 12:01:37 2016\n",
      "__log__021.txt                          210 Thu Mar 24 12:01:35 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/022/__log__022.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 022 AT 2016-03-24T11-01-37\n",
      "  2.20s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/022\n",
      "\n",
      "Bible_Chronica_II.gexf                 1229 Thu Mar 24 12:01:39 2016\n",
      "Bible_Reges_I.gexf                     1204 Thu Mar 24 12:01:39 2016\n",
      "__log__022.txt                          210 Thu Mar 24 12:01:37 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/023/__log__023.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 023 AT 2016-03-24T11-01-39\n",
      "  2.19s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/023\n",
      "\n",
      "Bible_Chronica_II.gexf                 1477 Thu Mar 24 12:01:41 2016\n",
      "Bible_Reges_I.gexf                     1954 Thu Mar 24 12:01:41 2016\n",
      "__log__023.txt                          210 Thu Mar 24 12:01:39 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/024/__log__024.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 024 AT 2016-03-24T11-01-42\n",
      "  2.33s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/024\n",
      "\n",
      "Bible_Chronica_II.gexf                 9982 Thu Mar 24 12:01:44 2016\n",
      "Bible_Reges_I.gexf                    15361 Thu Mar 24 12:01:44 2016\n",
      "__log__024.txt                          210 Thu Mar 24 12:01:42 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/025/__log__025.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 025 AT 2016-03-24T11-01-44\n",
      "  2.21s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/025\n",
      "\n",
      "Bible_Chronica_II.gexf                 2885 Thu Mar 24 12:01:46 2016\n",
      "Bible_Reges_I.gexf                     2629 Thu Mar 24 12:01:46 2016\n",
      "__log__025.txt                          210 Thu Mar 24 12:01:44 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/026/__log__026.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 026 AT 2016-03-24T11-01-46\n",
      "  2.24s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/026\n",
      "\n",
      "Bible_Chronica_II.gexf                 1770 Thu Mar 24 12:01:48 2016\n",
      "Bible_Reges_I.gexf                     1808 Thu Mar 24 12:01:48 2016\n",
      "__log__026.txt                          210 Thu Mar 24 12:01:46 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/027/__log__027.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 027 AT 2016-03-24T11-01-48\n",
      "  2.19s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/027\n",
      "\n",
      "Bible_Chronica_II.gexf                 1614 Thu Mar 24 12:01:51 2016\n",
      "Bible_Reges_I.gexf                     1731 Thu Mar 24 12:01:51 2016\n",
      "__log__027.txt                          210 Thu Mar 24 12:01:48 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/028/__log__028.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 028 AT 2016-03-24T11-01-51\n",
      "  2.18s Results directory:\n",
      "C:\\Users\\Frederik/laf-fabric-output/etcbc4/028\n",
      "\n",
      "Bible_Chronica_II.gexf                 2296 Thu Mar 24 12:01:53 2016\n",
      "Bible_Reges_I.gexf                     1731 Thu Mar 24 12:01:53 2016\n",
      "__log__028.txt                          210 Thu Mar 24 12:01:51 2016\n",
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-11-27T12-37-00\n",
      "  0.02s LOGFILE=C:\\Users\\Frederik/laf-fabric-output/etcbc4/029/__log__029.txt\n",
      "  0.02s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK 029 AT 2016-03-24T11-01-53\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from laf.fabric import LafFabric\n",
    "fabric = LafFabric()\n",
    "\n",
    "# which Bible passages to create co-occurence networks for\n",
    "# -1 matches the last chapter/verse. Useful when selecting a whole book/chapter\n",
    "\n",
    "# what range the co-occurence networks should have\n",
    "# creates new file and network at the start of a new \"chapter\", \"book\" or never when set to \"bible\"\n",
    "network_range = \"book\"\n",
    "\n",
    "# include / exclude named entitity types:\n",
    "# \"pers\" = person\n",
    "# \"mens\" = measurement unit\n",
    "# \"gens\" = people\n",
    "# \"topo\" = place\n",
    "# \"ppde\" = demonstrative personal pronoun\n",
    "# \"\"     = not specified (seems to be the 'gentillic' words, i.e. from which country someone is)\n",
    "allowed_nametypes = [\"pers\", \"mens\", \"gens\", \"topo\", \"ppde\", \"\"]\n",
    "#allowed_nametypes = [\"pers\", \"mens\", \"gens\", \"topo\", \"ppde\", \"\"]\n",
    "\n",
    "# minimum weight an edge should have before it is added to the output file\n",
    "min_edge_weight = 0.1\n",
    "\n",
    "# formula for the weight between words dependent on the distance in amount of sentences\n",
    "# weight = 1/(distance in amount of sentences^2)\n",
    "def get_weight(src_sentence, tgt_sentence):\n",
    "    return 1/(abs(src_sentence - tgt_sentence)+1)**2\n",
    "\n",
    "data_header = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<gexf xmlns:viz=\"http:///www.gexf.net/1.2draft/viz\" xmlns=\"http://www.gexf.net/1.1draft\" version=\"1.2\">\n",
    "<meta>\n",
    "<creator>LAF-Fabric</creator>\n",
    "</meta>\n",
    "<graph defaultedgetype=\"undirected\" idtype=\"string\" type=\"static\">\n",
    "<attributes class=\"node\" mode=\"static\">\n",
    "<attribute id=\"occurrence\" title=\"occurrence\" type=\"string\"></attribute>\n",
    "<attribute id=\"nametype\" title=\"nametype\" type=\"string\"></attribute>\n",
    "</attributes>\n",
    "'''\n",
    "\n",
    "for idx, passages_string in enumerate(allpassages):\n",
    "    passages = eval(passages_string)\n",
    "    \n",
    "    fabric.load('etcbc4', 'lexicon', str(idx+1).zfill(3),\n",
    "    {\n",
    "        \"primary\": False,\n",
    "        \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "        \"features\": (\"otype book chapter verse number lex_utf8 gloss sp ls nametype\", \"\"),\n",
    "    },)\n",
    "    exec(fabric.localnames.format(var='fabric'))\n",
    "    \n",
    "    # replace -1 in 'passages' dict with inf\n",
    "    for b, cv in passages.items():\n",
    "        for idx, item in enumerate(cv):\n",
    "            if item == -1:\n",
    "                cv[idx] = float('inf')\n",
    "        passages[b] = cv\n",
    "\n",
    "    books = []\n",
    "    unique_labels = []\n",
    "    unique_nodes = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda:0))))\n",
    "    def get_unique_lexeme(book_name, chapter_nr, lexeme):\n",
    "        if network_range == \"chapter\":\n",
    "            return \"{}_{}_{}\".format(book_name, chapter_nr, lexeme)\n",
    "        elif network_range == \"book\":\n",
    "            return \"{}_{}\".format(book_name, lexeme)\n",
    "        else:\n",
    "            return lexeme\n",
    "\n",
    "    correct_verse = False\n",
    "    node_id = 0\n",
    "    lexemes = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda:collections.defaultdict(lambda:0))))\n",
    "    edge_weight = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: 0)))\n",
    "    \n",
    "    for node in NN():\n",
    "        this_type = F.otype.v(node)\n",
    "\n",
    "        # when arriving at a new verse, check if that verse is in the allowed passages\n",
    "        if this_type == \"verse\":\n",
    "            correct_verse = False\n",
    "            this_book = F.book.v(node)\n",
    "\n",
    "            for b, cv in passages.items():\n",
    "                this_chapter = int(F.chapter.v(node))\n",
    "\n",
    "                if this_book == b and this_chapter >= cv[0] and this_chapter <= cv[2]:\n",
    "                    this_verse = int(F.verse.v(node))\n",
    "\n",
    "                    if cv[0] == cv[2]:\n",
    "                        if this_verse >= cv[1] and this_verse <= cv[3]:\n",
    "                            correct_verse = True\n",
    "                            #print('{} {}: {}'.format(this_book, this_chapter, this_verse))\n",
    "                            break\n",
    "                    elif (\n",
    "                        (this_chapter == cv[0] and this_verse >= cv[1]) or\n",
    "                        (this_chapter > cv[0] and this_chapter < cv[2]) or\n",
    "                        (this_chapter == cv[2] and this_verse <= cv[3])\n",
    "                       ):\n",
    "\n",
    "                        correct_verse = True\n",
    "                        #print('{} {}: {}'.format(this_book, this_chapter, this_verse))\n",
    "                        break\n",
    "\n",
    "        # if the verse is allowed, continue\n",
    "        if correct_verse:\n",
    "            if this_book not in books:\n",
    "                books.append(this_book)\n",
    "\n",
    "            if this_type == \"sentence\":\n",
    "                this_sentence = int(F.number.v(node))\n",
    "\n",
    "            if this_type == \"word\":\n",
    "                lexeme = F.lex_utf8.v(node)\n",
    "                nametype = F.nametype.v(node)\n",
    "                allowed_nametype = any(x in nametype.split(',') for x in allowed_nametypes)\n",
    "                unique_lexeme = get_unique_lexeme(this_book, this_chapter, lexeme)\n",
    "\n",
    "                # if part of speech == proper noun or lexical set == gentilic and nametype is allowed\n",
    "                if (F.sp.v(node) == 'nmpr' or F.ls.v(node) == 'gntl') and allowed_nametype:\n",
    "                    lexemes[this_book][this_chapter][this_sentence][lexeme] += 1\n",
    "                    occurrence = \"{}_{},\".format(this_book, this_chapter)\n",
    "\n",
    "                    # if the word is not yet in the chapter / book / bible, add it\n",
    "                    if unique_lexeme not in unique_nodes:\n",
    "                        unique_nodes[unique_lexeme][\"id\"] = node_id\n",
    "                        node_id += 1\n",
    "\n",
    "                        english_name = re.sub(r'\\W+', '', F.gloss.v(node))\n",
    "\n",
    "                        if [lexeme, english_name] not in unique_labels:\n",
    "                            english_names = [x[1] for x in unique_labels]\n",
    "                            while english_name in english_names and [lexeme, english_name] not in unique_labels:\n",
    "                                english_name += \"2\"\n",
    "                            if [lexeme, english_name] not in unique_labels:\n",
    "                                unique_labels.append([lexeme, english_name])\n",
    "\n",
    "                        unique_nodes[unique_lexeme][\"gloss\"] = english_name\n",
    "                        unique_nodes[unique_lexeme][\"nametype\"] = nametype\n",
    "                        unique_nodes[unique_lexeme][\"occurrence\"] = occurrence\n",
    "                    # otherwise add the occurence information\n",
    "                    else:\n",
    "                        if not unique_nodes[unique_lexeme][\"occurrence\"].endswith(occurrence):\n",
    "                            unique_nodes[unique_lexeme][\"occurrence\"] += occurrence\n",
    "\n",
    "    last_book = len(books) - 1\n",
    "    for idx_book, book_name in enumerate(books):\n",
    "\n",
    "        last_chapter = len(lexemes[book_name]) - 1\n",
    "        for idx_chapter, chapter_nr in enumerate(lexemes[book_name]):\n",
    "\n",
    "            # create the nodes in the graph\n",
    "            node_data = []\n",
    "            unique_labels = []\n",
    "            for node in unique_nodes:\n",
    "                if (\n",
    "                    (network_range == \"chapter\" and \"{}_{}_\".format(book_name, chapter_nr) in node) or\n",
    "                    (network_range == \"book\" and \"{}_\".format(book_name) in node) or\n",
    "                    (network_range != \"chapter\" and network_range != \"book\")\n",
    "                    ):\n",
    "\n",
    "                    node_data.append('''<node id=\"{}\" label=\"{}\">\\n  <attvalues><attvalue for=\"occurrence\" value=\"{}\"/><attvalue for=\"nametype\" value=\"{}\"/></attvalues>\\n</node>\\n'''.format(\n",
    "                        unique_nodes[node][\"id\"], unique_nodes[node][\"gloss\"], unique_nodes[node][\"occurrence\"], unique_nodes[node][\"nametype\"]))\n",
    "\n",
    "            # read all lexemes from the chapter and add them to the list 'names'\n",
    "            names = []\n",
    "            for sentence in lexemes[book_name][chapter_nr]:\n",
    "                words = lexemes[book_name][chapter_nr][sentence].keys()\n",
    "                # add sentence id to words like this: [sentence number, word]\n",
    "                # so the distance between the words can be calculated\n",
    "                words = [[s,sentence] for s in words]\n",
    "                names.extend(words)\n",
    "\n",
    "            # calculate the edge weights between the names in 'names'\n",
    "            for src in range(len(names)):\n",
    "                src_sentence = names[src][1]\n",
    "                lexemes_src = names[src][0]\n",
    "\n",
    "                for tgt in range(src + 1, len(names)):\n",
    "                    tgt_sentence = names[tgt][1]\n",
    "                    lexemes_tgt = names[tgt][0]\n",
    "\n",
    "                    if lexemes_src != lexemes_tgt:\n",
    "                        # set weight dependent on how much sentences the words are away from each other\n",
    "                        this_weight = get_weight(src_sentence, tgt_sentence)\n",
    "\n",
    "                        occurrence = \"{}_{},\".format(book_name,chapter_nr)\n",
    "\n",
    "                        # if there already exists a relation the other way around, swap vars\n",
    "                        if edge_weight[lexemes_tgt][lexemes_src][\"weight\"] > 0:\n",
    "                            if not edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"]:\n",
    "                                edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"] = occurrence\n",
    "                            elif not edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"].endswith(occurrence):\n",
    "                                edge_weight[lexemes_tgt][lexemes_src][\"occurrence\"] += occurrence\n",
    "\n",
    "                            edge_weight[lexemes_tgt][lexemes_src][\"weight\"] += this_weight\n",
    "                        else:\n",
    "                            if not edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"]:\n",
    "                                edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"] = occurrence\n",
    "                            elif not edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"].endswith(occurrence):\n",
    "                                edge_weight[lexemes_src][lexemes_tgt][\"occurrence\"] += occurrence\n",
    "\n",
    "                            edge_weight[lexemes_src][lexemes_tgt][\"weight\"] += this_weight\n",
    "\n",
    "            # if a network is complete, create the edges, export the file and clear the edge weights\n",
    "            if (\n",
    "                    (network_range == \"chapter\") or\n",
    "                    (network_range == \"book\" and idx_chapter == last_chapter) or\n",
    "                    (idx_chapter == last_chapter and idx_book == last_book)\n",
    "                ):\n",
    "\n",
    "                # create the edges\n",
    "                edge_id = 0\n",
    "                edge_data = []\n",
    "                for src in edge_weight:\n",
    "                    unique_lexeme_src = get_unique_lexeme(book_name, chapter_nr, src)\n",
    "\n",
    "                    for tgt in edge_weight[src]:\n",
    "                        unique_lexeme_tgt = get_unique_lexeme(book_name, chapter_nr, tgt)\n",
    "\n",
    "                        if edge_weight[src][tgt][\"weight\"] > min_edge_weight:\n",
    "                            edge_id += 1\n",
    "\n",
    "                            edge_data.append('''<edge id=\"{}\" source=\"{}\" target=\"{}\" weight=\"{}\" label=\"{}\"/>\\n'''.\n",
    "                            format(edge_id, unique_nodes[unique_lexeme_src][\"id\"], unique_nodes[unique_lexeme_tgt][\"id\"], round(edge_weight[src][tgt][\"weight\"],2), edge_weight[src][tgt][\"occurrence\"]))\n",
    "\n",
    "                # export the file\n",
    "                edges_header = '''<edges count=\"{}\">\\n'''.format(len(edge_data))\n",
    "\n",
    "                if network_range == \"chapter\":\n",
    "                    filename = 'Bible_{}_{}.gexf'.format(book_name, str(chapter_nr).zfill(2))\n",
    "                elif network_range == \"book\":\n",
    "                    filename = 'Bible_{}.gexf'.format(book_name)\n",
    "                else:\n",
    "                    filename = 'Bible.gexf'\n",
    "                out_file = outfile(filename)\n",
    "\n",
    "                out_file.write(data_header)\n",
    "\n",
    "                nodes_header = '''<nodes count=\"{}\">\\n\\n'''.format(len(node_data))\n",
    "                out_file.write(nodes_header)\n",
    "                for node_line in node_data:\n",
    "                    out_file.write(node_line)\n",
    "                out_file.write(\"</nodes>\\n\")\n",
    "\n",
    "                out_file.write(edges_header)\n",
    "                for edge_line in edge_data:\n",
    "                    out_file.write(edge_line)\n",
    "                out_file.write(\"</edges>\\n\")\n",
    "                out_file.write(\"</graph></gexf>\\n\")\n",
    "\n",
    "                # clear the edge weights for the next network\n",
    "                edge_weight = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: 0)))\n",
    "    close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
